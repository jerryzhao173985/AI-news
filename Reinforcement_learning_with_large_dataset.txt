 Today he's going to talk about this autonomous agents which is also a very popular and heated topic recently. We have seen some related talk in the past few weeks about how to build the language model for agents. But now I think we will see another perspective from reinforcement learning. So that's welcome and and then let's start. Thank you. Great, thank you for the introduction. All right, so we'll start with a little kind of thought exercise. Let's imagine that we are in charge of building an autonomous system to go and explore another planet. So we're going to build an AI system that will drive this like Mars rover so they can go and explore Mars, find interesting phenomena, report back on what it's found, maybe when it's finished it'll hop on a rocket and fly back to Earth. And it has to do all of this on its own. So it's going to be dropped into this unfamiliar environment. Maybe we know something about what it looks like, we certainly don't know everything. And then it's going to be out of contact with Earth. It has to be on its own and discover everything without human intervention. What can we do to prepare our autonomous agent for this unfamiliar setting? And we're going to talk, of course, about its brain. I don't know the first thing about building mechanics for a Mars rover, but we're going to mainly focus on how to build the brains of this thing so that it can solve this kind of task. So, of course, this is like a situation that people are uniquely effective at, right? Like if you imagine a story like Robinson Crusoe, a person gets stranded on a desert island and has to use all their resourcefulness, all their flexibility to figure out how to survive, how to find shelter and food. That's the kind of thing that we're dealing with here. Entering an unfamiliar situation, improvising as you go, discovering effective solutions to novel problems, and then coming out of it having accomplished your objective. So what kind of recent advances in AI can help us tackle this kind of problem? Well, the kinds of advances that we've seen, maybe we can put them into two categories, data-driven AI systems like generative AI, language models, image generation models, and reinforcement learning systems, things like AlphaGo. So can some ideas from these two areas help us build our Mars Explorer agent? Well, let's talk about the data-driven AI stuff first. We've seen some pretty remarkable results in this area, models that can generate images based on user commands, hold a conversation with chat, explain jokes, solve problems, write code, all that sort of stuff. But at some level, what all of these data-driven generative AI methods are really doing is essentially estimating probability distributions. And that's a very important thing to realize if we want to understand their limitations. And it could be an unconditional distribution, p of x, that's what a language model does. It could be a conditional distribution, p of y given x, that's what the image generation models do. But at some level, what these models are doing is a very, very large scaled-up version of the kind of density estimation that we learn about basically in statistics class. And whenever you're doing density estimation, whenever you're estimating probability distribution, of course, the question you have to ask is, well, what distribution generated your data? Because that's what you're learning about. And by training on lots and lots of data from the web, these generative AI systems are essentially learning about the kinds of pictures that people take or the kind of pictures that people draw and the kind of words that people type on keyboards. Now, this is not to understate the power of these systems. If you can model what people will type on keyboards, you have to understand a lot about people's behavior. But this also hopefully makes us appreciate that at some level, what these systems are doing is really emulating human behavior. And that's good because humans are very smart, but perhaps emulating human behavior is not always enough. So as a counterpoint to this, what do reinforcement learning methods do? Well, reinforcement learning methods are doing something actually pretty different. In reinforcement learning, you are not learning from imitating humans so much as you're learning through interaction with an environment. You have an agent that interacts with the world, collects some experience, modifies its behavior based on that interaction, and then repeats this process. And reinforcement learning methods have been effective in a range of domains, from playing video games to robotic control to, of course, things like AlphaGo. Now, the interesting thing about reinforcement learning is that it's a powerful engine for emergent behavior. What made the AlphaGo championship game so interesting to many people was this move 37 in the least hit all match, where the reinforcement learning algorithm discovered a move that surprised people. It was interesting because it was not a move that a human player would likely have made. So the generative AI results are so compelling to us because they look like something that a person might do, they look like something that a person might draw, or that a person might say. The reinforcement learning results are impressive precisely for the opposite reason, that they can produce results that a person would not do. They can take moves and go that a human player wouldn't make. Both are very interesting, both are very compelling, but for very, very different reasons. And that's what we're going to think about when we think about our Mars Explorer, because our Mars Explorer will need to exhibit generalization, it'll need to have a lot of that know-how that humans have, but it will also need to discover novel solutions to novel problems. Now, I might be suggesting here that reinforcement learning is kind of the better option, but it's really not because reinforcement learning also has a major limitation. While reinforcement learning is very good for finding... There's a little bit of echo here, thank you. It's very good for finding novel solutions in constrained environments like Go. It falls short of the data-driven approach when put in the real world, the world that has complex physics, a world that has interesting physical phenomena, lots of diversity, broad open world situations, and other complex things like other agents. These are all things that are actually very difficult for reinforcement learning to deal with. Reinforcement learning really shines in more constrained settings like the game of Go, where you don't have to worry about someone, let's say, knocking over the Go board and spilling the coffee or something like that. This is because reinforcement learning learns through iterative interaction, and iterative interaction with complex real-world environments is costly and expensive. That's why we want to mine all the data from the web to train our language models and our image generation models, because that data is already out there for us to take. If they had to go out into the world and gather that data from the web to train our language models and our image generation models, because that data is already out there for us to take. If they had to go out into the world and gather that data, they would not be quite as scalable. So there's a big gulf in reinforcement learning, not in terms of its ability to discover solutions, but in terms of its ability to generalize to real-world situations. So could we, for example, combine the best of these things? Could we have reinforcement learning that uses real-world data in an analogous way combine the best of these things? Could we have reinforced learning that uses real world data in an analogous way to the data-driven approaches? So the data-driven AI approaches, they learn about the real world from data. They observe lots of data typically from humans. They don't try to do better than the data. They're mostly trying to emulate the behavior of humans, but they can handle a lot of variability, a lot of open world generalization. RL methods optimize a goal with emergent behavior that can solve computationally very complex tasks like playing Go and discover novel solutions, but they don't make use of that real world data and therefore they're mainly constrained to these more limited environments with no rules. So data-driven stuff is all about using the data, reinforced learning is all about optimization. And data without optimization doesn't allow us to solve new problems in new ways. Optimization without data is hard to apply to the real world outside of simulators. So of course, what I'm going to talk about today is how do we get the best of both of these? Can we learn to make decisions with data? Can we learn to optimize but still bring in large data sets? Perhaps instead of employing the more widely used on policy RL framework, where you interact with the world, improve a little bit, throw out your data and track some more, improve further, could we have an RL paradigm that uses data, that can leverage large data sets of past interactions, pull out the best possible decision-making strategy that you can get from that, and then perhaps from there, you can very rapidly adapt to novel situations through much more limited and much more efficient online fine tuning. And maybe that will get us a lot closer to that Mars Explorer agent. Perhaps the notional recipe here would be that you would have data from your past experience here on Earth. Maybe it's things that your Mars Explorer tried to do in training. Maybe it's experience from humans doing things. Maybe it's even experience from other physical interactions that are not directly meant to simulate what you're going to see on Mars, but are meant to be kind of representative of the kinds of phenomena you'll encounter, navigation and off-road environments, manipulating objects, dealing with physical interactions. That's your past experience. That's what you can use to synthesize some initial strategy. Much like a human astronaut, when they're training, they're kind of learning behaviors. Those are not exactly the behaviors that they'll use when they actually go and explore another planet, but it gives them a really, really good starting point. And then off it goes. It goes to Mars and when it's there on Mars, it's going to keep learning. It's going to use that initialization built up from prior data, all that common-sense knowledge that it distilled out of its initialization, to rapidly adapt to whatever situations it encounters. So that when it actually encounters common sense knowledge that it distilled out of its initialization to rapidly adapt to whatever situations that encounter. So that when it actually encounters a novel phenomenon that it doesn't know how to deal with, it can rapidly discover a novel solution to this novel situation. Combining rich past experience and generalizable strategies from prior data with rapid online adaptation. So that's what we're going to strive towards. And what I'll tell you about today are some methods that we've developed and some experiments that we did that address different facets of this overall problem statement. So this talk is really about what we can accomplish if we combine data and optimization. And I'll split into four parts. I'll first talk about the fundamentals of offline RL, algorithms that can learn to make decisions using data sets. Now offline RL methods, of course, learn from offline data. In reality, we don't want to learn purely offline. We want to then also adapt rapidly through online adaptation, and we'll discuss that also. I'll talk about robotic foundation models that can be, that can leverage offline RL and online RL and other ingredients that will make them generalized to the real world. But then I'll also talk about applications of these ideas in other domains. I'll talk about how combining data and optimization can provide more effective generative models. And I'll also talk about how we can incorporate these ideas into language models and get agents that can interact more effectively with humans by leveraging both data and reinforcement learning. But first, let's start off with some algorithmic fundamentals. So first off, what do we actually expect offline neural methods to do? If we're given data and we need to learn policies from data, what can we expect? Well, one bit of intuition that sometimes comes up, but I think it's a little bit off base, is that this is a bit like imitation learning. Imitation learning is the kind of data-driven learning-based control that probably folks are most familiar with, where the idea is that if you have data that shows how to perform a task, you could emulate that data and then get a policy that performs that task. But in reality, offline RL is actually trying to do a lot more than that, although it can be shown that in some cases offline RL can be a more effective imitation learning methodology than previous imitation learning methods. But that's not really the focus of this talk. I think a better intuition is that offline RL is about taking much more chaotic data and getting better behavior out of it than what was shown. So imagine that you want to go from the green circle to the orange circle, and your data looks like this. So none of this data is actually very good for performing your task. But if you combine parts of what you've seen in the data in the right way, then perhaps you can discover a better way to perform the task than any of the strategies actually shown in the data. Essentially, the data shows you what you can do, and then you use offline RL to figure out what you should do to bring about the desired outcome. And the simplest way to understand this is with this kind of little stitching example, where if you've seen examples where you go from A to B, and you've seen examples where you go from B to C, you should be able to figure out that it is possible to go from A to C. But of course, the power of these methods is that this can happen at all different scales all at the same time. Perhaps you've seen a variety of different behaviors that are all suboptimal in different ways. If you get the best of each one, maybe you can be much better overall. If you've seen data from human drivers, and some drivers are good in some situations, some are good at others, nobody is good everywhere. If you can distill that into a driving agent that is as good as the best possible human in every possible situation, that will be then a superhuman driver. So by improving over the data in this way, using it to illustrate what can be done, and then figuring out what should be done with RL, you can actually do a lot better. Okay, so how do people actually design algorithms for doing this? What's sort of the algorithmic foundation of modern offline RL methods? I'll give a quick primer. This is not a very technical talk, but I will have a little bit of technical material just to give you a sense for how these algorithms are constructed. So first, we need to bring in a little bit of notation. In reinforcement learning, an agent interacts with a world, and the agent selects actions. And we denote those actions as A. And the world responds with states S and rewards R. And this continues for multiple time steps. The goal is to learn a policy, which is basically a probability distribution of actions given states, and a good policy is one that will maximize cumulative long-term reward. So not... Oh, is there a question? Okay, so not the reward that the agent would get for the immediate action that it takes, but the cumulative reward for the sequence of actions, so the long-term reward. And a very useful object for maximizing this objective is something called the Q function. So the Q function is a function of state and action that tells you that if you take the action A, t, in state S, t, and then follow the policy pi, what will be the total reward that you will accumulate? And if you can figure out the Q function for a given policy, there's a very straightforward way to improve that policy by being greedy, just taking an action with probability 1 if it is the arg max of the Q function. And this is the basis of policy iteration. You can also short circuit this process by directly updating the Q function to be the current reward plus the max over the next Q function. That's like plugging the argmax directly into your regression targets. And that's the basis of Q-learning. So if you just enforce this equation, this is the Bellman optimality equation for all states and actions, if you basically make the left-hand side equal to the right-hand side, then you will recover the optimal Q function, and from that you can recover the optimal policy. So typically, the way that people do this is they gather samples of states and actions, and they minimize the squared error between the left-hand side and right-hand side for all of their state action next state tuples. Now, this is a very well-known algorithm. It's three decades old at this point. The cool thing about Q-learning is that it doesn't actually assume that the samples you have, the states, actions, and next states, were generated by your latest policy. They could in principle be generated by any policy. So that's why this forms a really good basis for offline RL. But by itself, Q-learning is not enough to solve the offline RL problem. And this brings us to the main issue underlying offline RL methodology, the main challenge that offline RL methods have to deal with, which is the problem of distributional shift. And it wasn't obvious to people for a long time why this was such a big deal. In fact, even though Q-learning is three decades old, the importance of distributional shift for making offline RL work was only really realized about five years ago. So here's the problem. Let's take this Bellman equation and let's write it a little bit differently. Instead of writing it as a max over the next action, let's write it as an expected value. So you want to set the current Q value to be R plus the expected value of the next Q value in expectation under some policy that I'm going to call pi nu. And pi nu is that greedy policy. Pi nu takes the argmax action with probability one. So I didn't actually change the equation because the expected value under this argmax is exactly the max, it's the same exact equation, but this makes it a little bit more apparent that in order for Q-learning to work, these expected values under pi nu need to be accurate. They need to actually reflect the true Q value. And whenever you wonder if the value of some function in expectation under distribution is accurate, the question to ask is, well, what distribution was it trained under? Because if you train under the same distribution that you test under, then you'll have good estimates. But if they're not the same, then you will not have good estimates. So let's say that these target values, we call them y, these are your regression targets, the training objective is to minimize the difference between q and y in expectation under the distribution that produced your data set. So if you're doing offline RL, we call this the behavior policy pi beta, but it's basically that's just a shortcut for the thing that generated your data set. You don't know what that thing is, but we call it pi beta because we need to attach a name to it. So we're regressing to our target value in expectation under our behavior policy, and then we're testing for the value of that Q function in expectation under pi nu. So we expect good accuracy when pi beta is equal to pi nu. But of course that's never going to be the case because pi nu is this argmax. Pi nu is taking the action that has the largest Q value, pi beta was whatever generated the data, human behavior, random behavior, whatever you want. So typically pi beta and pi nu are not the same. And even worse, pi nu is selected to be this argmax, which means that it's a kind of adversarial example. It's actually finding the actions that might fool the Q function into outputting large values. And in practice, if we naively use Q learning in offline RL mode, we will see this overestimation problem. In this example, this is trading on the half cheetah task with offline data. The actual rewards are around minus 250, regardless of the data set size from 1,000 to 1 million. The estimated Q values go up to 10 to the 16th power. The y-axis here is a log scale. So you can see the Q function is massively overestimating because these argmaxes are finding these adversarial examples. So that's the distributional shift problem. So any offline RL method needs to basically address this problem in some way. And there are a number of different methods that have been proposed to do this, but they seem to share similar principles. The principles are first to use value-based methods, which basically means Q-learning or Q-functional active critic. And virtually all modern effective offline RL methods either do this or do model-based RL, which is also a kind of modeling approach with similar considerations. And second, somehow fix the distributional shift problem. And the distributional shift problem could be fixed in a number of ways. One very powerful principle is pessimism, which says for actions that are not seen in data, assume that they're bad. Another principle is to somehow constrain the policy to not deviate from pi beta too much. And then a third class of principles is to actually develop novel offline RL methods that entirely avoid ever querying actions that are not in the data set. This third class of methods is actually very interesting. Some of the most performant modern approaches are actually based on this principle, but I'll actually mainly focus on the first one, on the pessimism approach, because it's a little bit more intuitive in my opinion and a little easier to understand. But keep in mind that there are other principles that are very effective too, and if you want to learn about state-of-the-art methods for this, I actually highly encourage you to check out, for example, IQL, which avoids OOD actions entirely. But in terms of pessimism, I do think this is kind of a nice principle to understand. A lot of theoretical work on offline RL also centers around this. And it's actually quite a natural, in my opinion, and intuitive idea. So the idea is that if our challenge is overestimation, what we could do is this. Let's say that the green curve is our true function, the blue curve is our fit to this true function, the blue curve is actually a pretty good fit in most places. However, if we look for the places that maximize the value, we'll actually find exactly the mistakes in the positive direction, things like this. So what we can do is we can hunt down all of those overestimation points and we can regularize them to push them down. Now we don't know the true value of this out of distribution action, but we can say, if it's out of distribution, make its value low. And this can be done in a very simple way. The basic idea behind conservative Q-learning and pessimism is that we can take our usual Q-learning objective, that's the second line of this, and we can add a regularizer. And notice that this regularizer is minimizing the Q values under some distribution mu that is selected to maximize the Q value. So it's like an adversarial training approach. Find the high Q value actions and minimize their Q value. And that makes a lot of sense because if the problem is a little bit similar to adversarial examples, it makes a lot of sense that the solution should be a bit similar to adversarial training. Find those overestimated points and make their value low. That doesn't mean that we'll get the right value, it just means that we'll be conservative. It means that we won't overestimate excessively. And these kinds of approaches carry very appealing theoretical guarantees. If you choose your regularizer carefully, you can guarantee that you will not overestimate. And in practice, they can work very, very well empirically. Here is an example of conservative Q-learning in action for a multitask Atari example. So this is not quite our Mars Explorer yet, but it's getting at some of those similar principles. So what we're going to do is we're going to take 40 different Atari games, and we'll train a single Q function on all 40 Atari games using offline data. Now, the Q function we get from doing this is very good at playing the 40 different Atari games. In fact, it does significantly better than the average behavior in the data set. So here there's graphs showing two experiments, a suboptimal data experiment where the data from all 40 games is suboptimal, and a near optimal data experiment where you have expert demonstrations from all 40 games. And in both cases, the pink bar on the far right, the scaled Q-learning, which is a CQL-based approach, does significantly better than the average behavior in the data set shown by the dotted line. In fact, in the suboptimal data case, it's significantly better than actually any prior approach, including methods like decision transformers that have significantly more parameters, 200 million parameters versus 80 million. But that's not even the most important thing. I mean, it's nice that this thing can learn to play those 40 Atari games, but the interesting thing, thinking back to our Mars Explorer, is the ability for this multitask offline RL train model to then adapt to new problem settings. And it can adapt to new problem settings both from offline data and from online data. So here we took new games and we fine-tuned with offline RL initialized from that 40 game initialization and here the leftmost bar, the pink bar, that's the conservative Q-learning method and you can see that it does very well across these games in most cases, achieving the best results. And you can also fine-tune it with online RL so you can get a little bit of experience for a new game, in this case this is a new game variant, and you can rapidly fine-tune to that from the offline initialization. Here the right comparison is between the pink bar and the red bar. The pink bar and red bar have the same architecture but the pink one is initialized with offline RL, the red one is trained from scratch. And you can see that in some cases the pink bar is essentially infinitely better than learning from scratch under a data budget. So this is almost kind of a microcosm of our Mars Explorer, it's an Atari game explorer rather than a Mars Explorer, but it's kind of making that point that you can pre-train with lots of offline data, get a really solid initialization, and then rapidly fine tune. Offline RL and conservative Q-learning has been used in a range of domains, some of which I'll talk about today, like robotics and dialogue systems. But it's also been used for real world applications like mobile notifications, where it can optimize click-through rates, and digital marketing systems. So these are actually things that you can use with real data in the real world, and people have actually done this. But now let me talk about some of the interesting ways that we can use offline RL in practice and some of the practical considerations that come up. So I'll talk about uses in robotics, generative models, and language modeling, and let's start with the robotics examples. So if we want to build truly general purpose robotic systems, it's not enough for us to just have algorithms, right? We have a number of components we need to figure out. One of the big ones that we need to figure out is actually large scale data sets. So thinking back to that Mars Explorer, if it's going to learn from lots of data here on earth, it needs to actually have access to that data. So we need to get it somewhere. And in practice, if we're doing robotics, we have to somehow find it or make it. We also of course need these self-supervised algorithms. We can't necessarily assume that all this past data is for the tasks that we want. It might be general data interacting with the world. It might not have the right reward signal. Or even if it does, it might not be directly applicable to the tasks we want to solve. So we actually want self-supervised methods that can utilize it. And then, of course, we need flexible and general models that can adapt to downstream tasks. And if we can solve all three of these things, then perhaps we can figure out how to build effective large-scale robotic models, something that you might call a robotic foundation model that can actually address this. Okay, so that's what we're going to talk. And I'll start with an example in robotic navigation, because if you want to build an effective robotic foundation model, navigation is a good one to start with an example in robotic navigation, because if you want to build an effective robotic foundation model, navigation is a good one to start with, because while there's a lot of heterogeneity in navigation, there's also a lot of structural similarity. So to start with a data question, we assembled a data set of robotic navigation from a variety of different sources, because we want to train really general policies. And these sources include small-scale robots like RC cars all the way to full scale ATVs. So it's a very diverse range of different robots. And then we train a goal condition policy. So what this policy tries to do is look at the current observation of the robot sees, and it has a goal image. And then it tries to figure out the actions that will lead to that goal image and the temporal distance, the number of time steps. And the number of time steps is very similar to a value function. So later on, we'll actually connect this to RL and we'll replace that temporal distance with a value function. But for now, this is a kind of a supervised goal condition BC approach. We can combine this with topological maps and actually get it to solve long horizon navigation tasks. And the cool thing about this kind of model is when it's trained on data from many different robotic systems, it can actually generalize even to entirely new robots. So it can, for example, figure out how to fly a quadcopter indoors in hallways without having ever been trained on quadcopter data. Now I will say the quadcopter is pretending to be a ground robot because it's flying at a fixed altitude. But other than that, the model generalizes in zero shunt. And that's really good news for our Mars Explorer, because it means that maybe our Mars Explorer robot can use data from other kinds of robots to acquire the skills that it needs. If we scale up this kind of model with large transformers, so here we train a model that we call the visual navigation transformer on this data set, which has about 40 million parameters, not only can we get policies that generalize to even more robots like quadrupeds, we can also actually adapt it to downstream tasks. And that's one of the important capabilities that you want from these kinds of systems, is not just the ability to control things in zero shot, but just like with our Atari games example, the ability to rapidly fine tune to new kinds of tasks. And not all of those tasks might be goal image tasks. So one of the things you could do, for example, is you can take a pre-trained Vint model, replace the goal encoder with an encoder for a different task modality, like, for example, turn-by-turn instructions for autonomous driving, and get it to drive a vehicle in the carless simulator from a small number of demonstrations. So here, the instructions to the robot no longer correspond to goal images. They correspond to commands like turn left, turn right, go straight to the next intersection, the kind of stuff you might get from Google Maps. But the thing can actually successfully drive in Carla. In fact, a number of other labs have actually used the Vint model for their own research, including for things like controlling drones, off-road driving, driving through university campus and this is work that was done by other labs around the world, from Finland to Estonia to Princeton University. So the model can actually be used in other settings. And most recently, we were actually able to take our robots running the Vint model to the Bay Area Robotics Symposium and run it directly in the conference venue. We actually didn't train the robot around humans all that much. So I think it's thinking that all those humans are actually trees. It was trained a lot in forests, but it still avoids them pretty well. Now, of course, I motivate a lot of this by discussing offline RL, so I should tell you about how these ideas can be adapted to reinforcement learning. So one of the experiments we did is we modified the pre-training procedure to use offline RL on all this multi-robot data, and then we did rapid online modified the pre-training procedure to use offline RL on all of this multi-robot data. And then we did rapid online RL fine-tuning to a particular downstream environment, in this case, racing at high speed around hallways. Now, straight out of the box, the robot doesn't do that well. It bumps into walls a lot. But one of the things that it gets from that offline RL initialization is the ability to very rapidly improve. So after about 15 minutes of training with this new robot that was not in the offline data set, it's still bumping into walls, but it's making quite a bit more progress. And it's supposed to go as fast as possible around this track through the Soda Hall building in UC Berkeley. After a bit more training, after, let's see, after 25 minutes, now it can actually go around the hallway pretty well. And after about half an hour of training, it was actually able to race around the hallway at speeds that were comparable to what the best of our graduate student human drivers could do from the same first person images. So this is the first person view of the robot going around the hallways. This is after 25 minutes of training, initialized from this offline RL policy. So I think this is actually pretty remarkable, because for those of you that are familiar with RL policy. So I think this is actually pretty remarkable because for those of you that are familiar with RL from pixels, learning to drive a real robot from pixels in 25 minutes is way faster than we could ever do from scratch. And you can run this outdoors as well. This is an example. I believe this is the initial policy before training. So it doesn't do all that well. It kind of runs into things, but after about half an hour of training, it can do a pretty good job of racing around this outdoor track. And one interesting emergent behavior that has to go through these checkpoints, and the checkpoints are on the grass, but in between the checkpoints, it actually tries insofar as possible to drive on the paved path, because it knows it can go a little faster on the paved path. So it goes through the checkpoint here and tries to beeline back onto the paved path where it can drive a little bit faster. And here's the first person view from that experiment. OK, so that's pretty neat. That means that we can train with offline RL on data for many different kinds of robots, acquire that prior, acquire that physical common sense, and then fine tune from there with online training. Can we do all this for manipulation as well? Now, robotic manipulation is a little bit more challenging in this regard, because for manipulation, you have a lot more variety, a lot more physical variability in the environments you interact with, so you need to take a lot of care to get the right data set. We assembled this data set that we called a bridge data set, which has over 60,000 trajectories, hundreds of objects, full language annotations on 24 different environments. And we were able to actually validate that if you train policies on the bridge data set, they will run in other labs. So we got our colleagues at Stanford to run these policies, and they can successfully run in other laboratories, having been trained on data from the environments that we collected. Now, the cool thing here is that just the data alone already enables pretty impressive policies. So these are goal condition policies trained with a very similar method to the Vint model. And this is a single goal condition policy that can perform all of these tasks. There isn't actually anything particularly novel about the policy itself. It's basically the same approach as the one I discussed for navigation. But by giving it the right data, it can perform sweeping, close drawers, manipulate cloth, that sort of thing. And we can also train language condition policies. And they can also carry out a wide variety of different language instructions. So having the right data set here makes a really big difference. But then from there, we can also incorporate offline reinforcement learning. And one strategy that we've been exploring here that seems to be quite effective is to actually use offline RL, not just with diverse robot data, but also include video data into the mix. So what we do is we actually use video data from the Eagle4D dataset to train value functions using a self-supervised RL approach called ICVF. ICVF stands for Intentional Conditioned Value Functions. I won't have time to go into the detail, but it's essentially a generalization of goal condition training. Goal condition training trains to reach goal images, ICVF trains to reach latent outcomes. But it's a similar principle, it's learning how to go from one situation to another, and it's using the a similar principle. It's learning how to go from one situation to another. And it's using the video data to essentially predict the value, to predict reachability, which outcomes can be reached from which state. That gives us a really good initialization for the value function, which we can further train on diverse robot data from the bridge data set. And from there, we can then adapt it with just 10 to 20 trials to a downstream robot task, like putting the pot in the basket or something like that. So here are some examples of the tasks, the actual downstream fine-tuning tasks for this method that we call VPTR. And some of them involve manipulating rigid objects. Some of them involve opening doors. Some of them involve more complex tasks like sweeping granular media. These are difficult tasks. They have a lot of distractors. They have distributional shift from the pre-training data. So the success rate of VPTR shown in red here is about 50%. But that's still a lot better than the alternative methods, and it's much better than methods that actually don't use RL. A particularly interesting comparison can be seen in these two rightmost columns. The rightmost column is VPTR without the diverse robot data, without the bridge data, using only videos. And the second to last is without the video data, but using only the diverse robot data. And you can see in both cases, the performance plummets. So that means that VPTR is getting a lot of knowledge, both from the prior video data and from the diverse robot data. And that's, again, really nice for our Mars Explorer, because that means that there's hope that our Mars Explorer can do offline RL pre-training not just on robotic data that it gathered itself but also on data that gathered from other sources including videos of humans. So that's really promising. Of course we can scale up robotic manipulation even further. One very recent effort that we have is to combine bridge data with many other sources of data to form a kind of federated diverse robot data set. So in this effort called RTX, we combined data from 60 different data sets with 22 different kinds of robotic manipulators from 34 different research labs at 21 different institutions. This led to a data set containing about 1 million different trials in all sorts of different environments for all sorts of tasks and crucially with all sorts of different robots. And this data set could be used to train a single policy that could control the robots in this data set. So this is a single policy running on all of these robots around the world from UC Berkeley to Stanford to Freiburg. And the policy successfully performing, in this case, language condition tasks in all of these settings. In fact, when we actually asked the different labs that contributed data to RTX to test their policy in comparison to the multi-robot policy, we found that in most cases, the multi-robot policy actually did better than whatever policy they designed for their own tasks, and on average, an improvement by 50% on their tasks over their strongest baseline. So that's pretty impressive. That suggests that we could actually build these multi-robot policies and they could be viable in the real world. In fact, we also can scale up the underlying offline RL methods. We have not combined offline RL with RTX yet, but we have developed offline RL methods that can be applied to models of comparable scale, these kinds of transformer models that we use with the RTX experiments. So the work here is called Q-Transformer. And the idea in Q-Transformer is essentially to perform token-by-token offline RL training with discretized actions. So the policy architecture that was used in RTX discretized the action space and turns the action dimensions into tokens. Q-Transformer takes these tokenized action representations and computes Q values on individual action tokens. Now each token is a dimension of an action space, so essentially each dimension becomes a separate time step. So this is a much larger RL problem, but it is a discrete action problem which actually makes it easier in some ways. And Q-Transformer can learn language condition policies. This was still done in a single robot setting, but it performs significantly better than the imitation learning approach. So the right comparison here is the rightmost column. Q-Transformer has an average success rate of 56% on these challenging out-of-distribution generalization experiments. The RT1 model, which is the one that was used in the RTX experiments, has less than half the success rate. Other methods do a little bit better than RT1, but Q-Transformer does significantly better. So what have we done, and what remains open? Well, we've shown that robotic learning from multi-robot data sets works and enables models that people around the world can use. So you can actually generalize, both for navigation and for manipulation. You can build a single model that can control many robots so that can be used by people around the world. Offline RL can enable more performant policies and can enable rapid online RL fine tuning. It can learn from videos. The online fine tuning can be done in tens of minutes. And this seems to hold for both navigation and manipulation. However, the largest models still use imitation learning and integrating all the components together has yet to be done. Advances in more scalable RL methods are important there, so maybe things like QTransformer can give us a leg up, maybe we can push that through and get it to train RTX. We also need to be able to evaluate rewards at scale, so the RL methods require reward functions, goal-conditioned rewards can bridge part of that gap, maybe sometimes goal-conditioned rewards are too sparse, and we need more detailed reward functions. But I think we're right on the cusp of being able to address these issues. And I think that in the next year or so, we'll see powerful offline RL methods actually become the dominant standard for robotic pre-training, such that whenever you're doing a robotic learning experiment, you might actually start your experiment from one of these pre-trained robotic foundation models in the same way that in NLP, people start their experiments from pre-trained language models. So that's the robotics side of the story, but I think there's a lot more to combining data and optimization. in NLP, people start their experiments from pre-trained language models. So that's the robotics side of the story, but I think there's a lot more to combining data and optimization, and I'll talk about some other application domains next that might get us thinking about this. First I'll talk about maybe an unlikely place where we can see a lot of impact from RL, and that's generative modeling. So I said before that the kind of advances we've seen in generative AI are all about using data, and they're all about emulating human behavior, whereas RL is about emerging behavior. So so far, we talked about how to make RL and control better by leveraging data, but can we make the data-driven generative models better by leveraging RL? That might seem a little bit strange at first, because if we want to generate pictures, well, it actually is very appealing to generate the kind of pictures that people might draw. Like, people draw pretty great pictures, so if we can just copy that, we'll do pretty well. But the problem is that we don't just want the pictures that already exist. The whole reason we want AI systems to do image generation is to generate the kind of pictures that haven't been made yet, to generate the novel creative pictures. And oftentimes, the novel creative pictures that we want to make are ones that are a little bit unlikely, that are a little bit out of distribution. Let's say that someone asks you, make me a picture of a dolphin riding a bike. Well, if you ask stable diffusion 1.4 to generate an image for this prompt, this is the picture that will generate. I think newer models will probably do quite a bit better. I'm sure that DALI 4 will do a decent job of this, but this is what we got out of stable diffusion 1.4. And why is that happening? Why is the dolphin not on a bicycle? Well, the issue is that it's a little bit like that MOVE 37. It's like the artist's version of MOVE 37, in the sense that a dolphin riding a bicycle is like a pretty weird thing. So if you simply ask the model, what kind of picture do you imagine you might find on the internet? Well, this might be the closest thing that I can think of. So we really need optimization to get emergent behavior here to get something that really accords with that prompt, that really satisfies dolphin riding a bicycle, rather than necessarily matching the distribution of images that we've seen on the web. So here's the idea. We're going to use RL to optimize the image. We're actually going to optimize an entire model based on a distribution of prompts to create images that accord with a particular prompt distribution. And the reward function for this will actually be provided by a vision language model. So we have a distribution of our prompts. These were specified by hand through a grammar. So in our case, the prompts will all be animals performing activities, monkey washing dishes, dolphin riding a bicycle, that sort of thing. And that's just for testing. That goes into a diffusion model. And then a vision language model is asked what is happening in the image. And it provides an answer. And then we're going to use BERT score to compare that answer to the prompt that went in. Now, this might seem a little strange because these are all learned models. So where's the ground truth signal coming from? Well, the idea is that the image generation model is aiming to emulate the distribution seen in the data, whereas the visual language model is really teaching you to solve the task rather than copy the data. So while there's not necessarily an additional source of knowledge beyond internet data, there is the knowledge that we are actually trying to solve the task, that our goal is really to match that prompt, not necessarily to emulate the distribution of the data set. And that is actually hidden knowledge in disguise. So let's see what this optimization process looks like. So first, I'm going to describe the algorithm, and then I'll show you an example of how this actually modifies an image. Now, the technical challenge with deriving this algorithm is that diffusion models are not easy to optimize with RL. And the reason is that diffusion models don't readily provide log-like results. They provide a mechanism for producing an image, but it's very hard to ask them, what is the likelihood of a given image? More precisely, if we're doing reinforcement learning, those of you that are familiar with policy gradient methods, what we really need out of our model is the ability to compute the gradient of the log probability of the likelihood of a data point. So in the policy gradient formula, we really need grad log pi. What we have instead in a diffusion model is the gradient of the log probability of a particular diffusion model is the gradient of the log probability of a particular diffusion step, the probability assigned to going from a future step in the diffusion process to a preceding step, the probability of a denoising step. So what we can do instead is we can develop a new algorithm that performs RL where every step of the diffusion process is treated as a time step of an MDP. So the image generation process is actually treated now as a sequential process where every step of diffusion is a time step. And that actually lets us apply the standard policy grading formula where the reward is given on the final image, so it's kind of a sparse reward problem, and every single time step has a policy making a choice about how to denoise. And that now lets us apply powerful RL methods like PPO to image generation. It actually works really well. It works significantly better than alternative approaches. So the blue curve here is the full PPO style importance sample version of our method. The purple is a more naive, reinforced-based method. And the red and orange are baselines based on prior work. And there's some concurrent work that also explored these ideas for other kinds of image generation challenges. But now let's see how this actually works in a particular example. So we're going to be looking at the evolution of this image dolphin riding a bike. I will say we are not just optimizing one image, we are optimizing an entire model on a distribution of problems for animals doing different things, and I'll just show the evolution of this one image over the course of training. So this is the initial image. This is passed to lava which produces a reward and after a little bit of optimization this is the image we get. So there's a dolphin and there's a bicycle although the dolphin is not really interacting with a bicycle in an interesting way. After a bit more optimization we get this image. Now this is getting a little bit more interesting. There's some creature that might be a dolphin and it's definitely on a bicycle. After more optimization, there's some more dolphin-like cues. And at the end, this is the image that we get. So you can see the image is actually changing very substantially. And we're getting something much closer to the prompt. And here are more examples, dog riding a bicycle, shark washing the dishes, all sorts of things. One of the interesting phenomena we notice is that the images get kind of cartoony. We think that's maybe because children's books have more examples of animals doing activities, but they definitely all get much closer to the prompt, much more recognizable as the activity that the prompt describes. And the other cool thing is by optimizing for a variety of different prompts at the same time, some prompts where there's very little success initially, like the dolphin riding a bicycle, are actually lifted up by the other easier prompts, like monkey washing dishes, where there's a lot more signal early on. So actually optimizing a distribution of prompts works a lot better than optimizing a single prompt. And in fact, after we train a model with RL in this way, we can then test that model on new out-of-distribution prompts like a parrot driving a car. Now, these are not prompts the model was trained on, but you can see that it actually gets better even on those unseen prompts. All right, the last topic I'll talk about is how we can also apply some of these ideas to language models. And that's a really exciting application domain because language models are already very powerful, but offline RL can make them much more goal directed. So can we take data of human dialogues, put that into some kind of training procedure and get an agent that is better at interacting with humans than the humans themselves are? Can it essentially, instead of just copying human behavior from the dialogues, can we actually optimize for outcomes? Can we analyze those dialogues, figure out what it takes to interact with a human effectively, and then essentially have like a superhuman dialogue agent? What should go in that box? Just copying humans doesn't necessarily lead to great performance, but just like before, the data tells us a lot about how the system works. It tells us what we can do, and then we can use RL to figure out what we should do. So can we leverage those patterns to do better than humans? Now, one approach that has been used for this is RLHF. RLHF is one way to do RL with language models, where the idea is that you will train a reward model where you will ask humans, is this example better than this other example? And humans will give you preferences. They'll tell you which one they think is better. So like actually answering the technical support question is better than just making fun of the user. And then you'll optimize that reward model. I think that's not quite enough though for what we want because while this is a good way to align models with human preferences, humans are not necessarily going to be better at expressing preferences than they are at actually doing the task. So this is good if we want models that do what humans want them to do, but it might not be enough if we want models to do better than how well humans can do. So if the goal is not to say a particular thing, but to achieve a particular outcome, then we need a different RL framework. What we can do is we can frame the human interaction, the dialogue itself, as a sequential decision process. So this is an example of a benchmark task called visual dialogue, where two agents talk to each other, and at the end, the questioner agent is supposed to guess which picture the answer is thinking of. It's a nice illustrative example that we can work with. And here, the questioner gets observations, which are the statements of the answer. And it takes actions, which are the questions that it asks. So this is a sequential decision process. There are observations, which are the answer statements. And there are actions, which are the questioner's questions. The action is what the bot says. the observation is what the human says, and the state is the history of the conversation. And then the reward is the outcome of the dialogue. The reward is received at the end, just like in that image generation example, the final image, or in this case, the final answer, is what determines the reward. This is not RLHF. In RLHF, we learn from preferences from humans. In sequential decision making, we learn from dialogue outcomes. In RLHF, we learn from preferences from humans. In sequential decision-making, we learn from dialogue outcomes. In RLHF, an episode is a single utterance. In sequential decision-making, it's all about the sequential nature of the task. It's about saying things now that would get you an answer, that would help you answer the question correctly later. So you really need to reason about the unpredictable behavior of the other human speaker to answer the question properly. So perhaps what we can do is we can put offline RL in this box. Perhaps we can take lots of data and actually optimize for a desired outcome using a method like Q-learning. So here's how we could formulate this dialogue task as a Q-learning problem. What we're going to do is we're not actually going to treat each utterance as an action. We'll actually treat each token as an action. Now, imagine that tokens are words. I know tokens are not actually words, but if they're words, it's a little easier to understand. Pretend we're at the word facing. We're looking at this, this is a static dataset. We're training on it with our language model, and we're at the token representing facing. We're going to train our transformer value function. The previous word was they. We output in a regular language model, we would output a distribution of our tokens, but here we're actually going to output a value over tokens. But just like a distribution, a value over token is just a number. It's just that unlike the likelihoods, that number has a different loss on it. So what we're going to do is we're going to take the number corresponding to the next token, which is facing, and we're going to have a loss, which is just the Q-learning loss. So we're going to take the max over the next token, that's the target value, and then we're going to add the reward to it which is going to be zero everywhere except the end of the conversation, and that's going to be the target value, in this case 0.8, and our loss will just be the difference between the current value and the target value. So this is a very different framework, it's Q-learning rather than supervised learning, but the structure is actually very similar to the supervised fine-tuning approaches that we already use for LLMs. It's just that instead of a likelihood loss, we have a Q-function, a Q-learning loss, which uses the value of the next token and then combines it with a reward. This is also exactly like how the Q-transform method from before worked. And we can compare this Q-learning approach for fine-tuning language models to supervised fine-tuning. In a language model, what we're fine-tuning is the probability of the next word given the previous words, or the probability of the next token given the previous tokens. For a value function, it's the probability of success of the conversation given the previous tokens and the new token. But it's still just a number. The previous tokens are the state, the new token is the action, and success is the Q value. So even though the method is conceptually very different, it works actually on very similar principles implementation-wise. Here are some examples applying offline a relative visual dialogue. This is from a method called ILQL. And one of the cool things we can do is we can actually vary the reward function and get different behaviors using the same exact data set. So here the answer, the questioner is the bot, and when it's just optimized to perform the task, it asks a lot of yes or no questions. We can modify its reward function and actually get it to ask more informative questions. So here the reward punishes it for yes or no answers, so it's not punishing it for asking particular questions, it's punishing it for getting particular answers. So it's trying to avoid yes or no and now it starts asking more informative questions. We can also have an even more stringent penalty where it gets penalized for answers that are yes or no and also answers that are kind of nebulous like can't tell, don't know, maybe, and now it starts asking much more quantitative questions. So you can see its behavior is actually being really tailored to the form of the reward function. And this actually does quite a bit better than supervised fine tuning as well. But now we have another problem. If we really want this to be an effective way to train dialogue agents, we often actually don't have good data for the particular dialogue tasks that we want to solve. Just like with our Mars Explorer agent, we might have a lot of data that can teach us about common sense, data from other scenarios, other kinds of dialogues, other kinds of tasks, but we might not have a lot of data for the particular domain that we want to tackle. So maybe we're trying to train a bot that will teach humans concepts, maybe it'll teach human students about RL and behavior cloning, and what we want is a bot that will ask lots of clarifying questions. It might say like, oh, to start with, tell me if you're familiar with AI, have you ever heard of machine learning? And then tailor the explanation accordingly. If you don't have good data, maybe what you could do is you could ask GPT-4. Tell GPT-4, be a good educational agent and ask lots of clarifying questions. Well, this is what GPT-4 will do. be a good educational agent and ask lots of clarifying questions. Well, this is what GPT-4 will do. It'll give you one of these classic GPT-4, chat-GPT-style answers, which is a giant wall of text. It's trying to be helpful, but it doesn't understand the conversation is a sequential process, so it just dumps all these questions on you. It essentially gives you a big questionnaire that you have to answer without any regard for whether that's actually a good next step to then eventually teach you the concept. So if you don't have good data, just prompting a giant model is not good enough. However, that giant model probably does already know quite a bit of knowledge that would really help you with the RL process. So maybe instead of prompting it, you could actually use it in combination with offline RL. So here's what we could do. We can essentially adopt a model-based RL approach. We can have a human construct a prompt, but then not actually use this prompt to create the agent directly, but instead use this prompt to create data. Ask a language model to pretend to be a teacher and a student. And then it'll essentially act like a human simulator. And that's much easier for it to do than to actually be a good teacher, because it doesn't have to necessarily be a great teacher to be a human simulator. And that's much easier for it to do than to actually be a good teacher because it doesn't have to necessarily be a great teacher to be a good simulator. It just needs to produce a variety of dialogues, some of which are good, some of which are bad. If the dialogues are not interactive enough, it can actually refine the dialogues. It controls both sides of the conversation. It actually can be primed with some initial prompt about the student. You can say, well, imagine that you are a very smart student. Imagine that you're a very knowledgeable student. Imagine that you're a very knowledgeable student. Imagine that you're a very impatient student. And it can generate plausible dialogues for all of these personas without necessarily knowing what it takes to be a good teacher. So it's just generating how humans might behave. It's generating examples of what could be done, not necessarily what should be done. And then what should be done will be determined by RL, which will analyze this data set, and then actually create a more optimal agent that teaches the concept more effectively. So just like before, we're using offline RL, where data tells us about what's possible, and RL tells us about how to be more optimal. And this is a really good way to leverage a large language model, because LLMs do what they're good at, which is simulate the behavior, and RL does what it's good at, which is optimization, data plus optimization. So with RL, instead of emulating humans, LLMs can simply learn to achieve desired outcomes using their understanding of how humans behave. So after we do this procedure, we can compare before and after. So here is a GPT agent with the best prompt that we could design to get it to be a very interactive teacher. And it's still generating these kind of, you know, verbal vomit gigantic GPT style outputs where there's way too much information at each step. And here's what the RL optimization does, much more concise questions. Of course I'd be happy to explain behavior column to start, but you tell me if you ever come across the term artificial intelligence? And the one the person responds is, oh, no problem at all. Let's take it step by step. Have you ever used a computer or a smartphone? So now we've finally gotten to that point where it's actually the LLM that's telling the human to think step by step. I think that's very nice. All right, so just to conclude, this talk was about what we can accomplish if we combine data and optimization. And we saw that we could do this in robotics for generative modeling and in the domain of language models. But for a little bit of perspective, I wanna end with a note about something that many of you might be familiar with, which is Richard Sutton's essay about the bitter lesson. For those of you that don't know about it, Richard Sutton had this really wonderful essay a few years back that articulated what has become a kind of a mantra in modern deep learning. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The idea is that we should focus on scalable learning methods rather than overthinking the particular details of inductive bias. And a lot of people took this to mean that basically the way to make machine learning work is to just shovel more data into more GPUs. And that's a very appealing prospect, but if you read this essay very carefully, it actually says quite a bit more than that. It says that the two methods that seem to scale arbitrarily are learning and search. And notice that he says only two, and those are the two that he picks, learning and search. And what does that mean? Well, to really understand the meaning of this, this is a little bit of a term of art. In RL, search means a very particular thing. So we all know what learning means. Learning means use data to extract patterns. Search basically means use computation to extract inferences. The difference is that learning is about figuring out something about the world. Search is about making deductions based on what you've already learned. Search is essentially optimization. So really the bitter lesson is that we need both learning and optimization, data and optimization together, to get intelligent scalable systems. Search is some optimization process that uses typically iterative computation to make rational decisions. Learning allows us to understand the world. Search leverages that understanding for emergent behavior. So data without optimization doesn't allow us to solve new problems in new ways. Optimization without data is hard to apply to the real world. If we have both learning and search, then we can really get something that can solve perhaps our Mars Explorer scenario. And to get a little bit philosophical on this, we can ask questions like, why do we need machine learning in the first place? To try to answer that question, we can step back and ask an even more basic question, why do we need brains? The famous neuroscientist Daniel Wolpert said, we have a brain for one reason and one reason only, and that's to produce adaptable and complex movements. Movement is the only way we have affecting the world around us, and to understand movement is to understand the whole brain. By analogy to that, we could say that we need machine learning for one reason and one reason only, and that's to produce adaptable and complex decisions. We don't really want predictions. We don't really want decisions. And prediction is just an intermediate step towards decisions. So it really makes a lot of sense that if we want truly capable machine learning methods, we need to be thinking hard, not just about data and prediction, but also about decision-making. Decisions could be everything from moving your joints or driving a car all the way to predict to computer vision. Is a decision to predict a label? Maybe it's what happens afterwards. Do you detect an animal in a camera trap? Do you detect an intruder in a security camera? Do you detect a person's face on Facebook? How does that affect your business downstream? These are all decisions, and decision making mechanisms can help us do that. And we can generalize this idea. We can use lots of diverse data from lots of past interactions to run a self-supervised RL procedure. And we saw examples of this. Could be everything from self-supervised discovery to goal-conditioned RL, and then rapidly adapt to the downstream tasks. And all flat RL gives us good tools for that. So perhaps combining data-driven learning with control and optimization will really help us build much more powerful self-supervised pre-training methods that put decision making at their core. All right, thank you very much for listening and I'd be happy to take questions. Thanks for the great talk and let's see first if we have any questions in the room. Yes. So first off, I'm not sure whether I'm nodding off, but I have a question regarding violating the offline URL and that one. You are at least background in first grade. So why don't you do measure a lot of offline learning instead you have to keep your digital shift in view versus, you know, we keep our data in our offline data. It doesn't really cover any generalized or outside of the law situations. But one thing I kind of was thinking about is in dialogue, usually our environment is different, right? So whether it's something either with the task on Ubuntu or, you know, whether we're trying to communicate with the user to buy certain products or something like this. Usually, the environment is very fantastic in a way that they're represented by humans. And it's just kind of an outside view. During this process, we use a large-scale model in the cloud to simulate humans. Then you don't realize that you're trying to simulate a kind of very good, the problem is people are human, they don't realize that the type of people is kind of very good, tacit environment, easy to use, and you know, a pretty single-minded model. So I think that... I'm sorry, I actually can't really hear it at all. Yeah, I will... I think Jingfeng might have trouble summarizing that question. I wonder if you could come up to the microphone. I think there's a microphone on the podium or something. Sure, yeah, would you like to come here and speak to the microphone? something. Sure. Yeah, would you like to come here and speak to the microphone? Like this microphone. Yeah. Yeah, I was trying to summarize that later, but it's about it. I mean, it sounds like a really interesting question. I caught a few words of it, but I worry that you might not be able to summarize it. Yeah, please go ahead. Hopefully, it's better this time. I'll try to be quick and ask. Yeah, I can totally hear you now. Please go ahead. So the idea was that, because I work a little bit on dialogues, I feel like one unique thing about dialogue compared to other sequential decision-making tasks is because we're dealing with humans as our environment. So whether it's convincing a user, teaching a user to do something, or having a user on Ubuntu forums, we're dealing with a very stochastic environment. And one way you mentioned this, when we're trying to do a reinforcement learning in this setup, maybe we can use large language model as a leverage, right? Whether it's prompting large language model as simulators or we prompt it with better inductive bias than do imitation learning. But one kind of concern is like, how do we make sure this kind of simulated environment has good fidelity to normal? But I think I agree with you that this should be like a really good starting point. And actually, I'm kind of doing similar trials, but one thing I kind of noticed is like, sometimes it's really hard to kind of make sure this is well aligned with human behavior in the wild. So have you considered any kind of task to make sure the fidelity works, or do you have any insight from algorithm slash data perspective? Yeah, I think that's a really good question. And maybe the way that we could formalize this in RL language is we could say, well, the task has a lot of hidden state. There's a lot of hidden state in the human's brain, essentially, that will determine how they will behave. And that's actually, I think, part of the reason why in that model-based approach I mentioned, it actually worked a lot better to have the language model actually generate both sides of the dialogue because then it knows the hidden state. But it's a big challenge. And we did find that we had to essentially prompt in the right way to get a generalized behavior that was appropriate to the situation. What we're studying right now is how to do essentially a few-shot adaptation. So what if you do have some dialogues from the target domain, but not enough to train a good policy by itself? Can you use that to essentially figure out that hidden state, then amplify that data with lots of synthetic data from the LLM and then run RL. And I think that could be a really promising direction. I think another really promising direction could be to do what we did in robotics and actually do some online fine tuning. So essentially figure out some possible ways that you can interact with humans from the offline data with offline RL, and then do very rapid online adaptation. And that could really help bridge the gap. I see. Yeah, thanks for the great answer. And I actually have a similar question, but on a parallel direction. One thing is that large language models are really strong these days. And in a way, this seems to can be a very strong prior, where we sample potential action from them. And usually, they're kind of reasonable. I think a lot of people call them common sense reasoning. So have you thought about trying to leverage these prior inferences from a pre-trained model and then do something on the downstream? Or would you say it would be better to just train an RL foundational model to do these kind of inferences? Yeah, that's a really interesting question. I think that this is actually something where there's a lot of room for controversy. So the last few slides in my talk sort of implied that maybe we could even do the pre-training with RL in the long run. On the other hand, supervised pre-training by next token prediction works really well. So it's not clear if that's really needed, but it could be really interesting potentially. So one thing you could imagine is RL in a sense learns about counter factuals. Like it learns if I were to take a different action, how would the outcome change? I could easily imagine that might be a better pre-training scheme than just direct prediction, but I don't really have any evidence of that. I do think that's a really interesting question. I don't have a much better answer for you, though. I see. So a final quick question is that, so one thing I noticed is offline RL usually, or at least like throughout your talk, the data sets are relatively large. So have you seen any evidence on like, without pre-training just emerging to a few-shot behavior in more kind of offline RL setups? Without pre-training, certainly not for language models. I mean, you could do it in very constrained settings. Like if you want to train a transformer essentially to play tic-tac- I mean, you could do it in very constrained settings. Like if you want to train a transformer essentially to play tic-tac-toe, you can do it. I do think that there's a lot that could be done to improve sample efficiency in the offline setting. That said, I'm not that concerned about it because I do think that in the long run, what we'll probably want to do is pre-train on large data sets. And I think that the frontier that we should push is to enable using large data sets that are less relevant to the task, but still big. I think that the frontier that we should push is to enable using large data sets that are less relevant to the task, but still big. I think that's probably a better recipe than trying to train from scratch on narrow task-specific data sets. That said, I do think there's research that could be done in both directions. I see, yeah. So the HDMI cable breaks for a couple of seconds, but I think I get the full picture of your answer. Thanks again for the great talk, and thanks for the great talk. Are there any other questions? No. Great. I think there's one over Zoom chat. It's saying, yeah. Oh, yeah. I see it. I have one question. Basically, I'm interested in how you try to introduce a foundation model for robotics, which enables doing several tasks with one policy. My question is, how could you make guarantees on the performance and safety? Yeah, that's a really good question. I think in general, when it comes to robotic learning, safety is a pretty major consideration, but I don't think anybody has a great solution for it. My own take on this, though, is that I think in the long run, safety mechanisms are also going to be, to a large extent, data-driven. And I think that this is a place where actually diverse data from many different situations can help you provide for a degree of safety. I don't think we're definitely out there yet, but that's my take on the long run. And the way that you could imagine this working out is that, essentially, if you've seen a variety of different behaviors, you kind of know that in these situations, these kinds of reasonably reliable things could happen. In other situations that you haven't seen, kind of all bets are off. So in a sense, the more stuff you've seen, the larger your region of understandable interpretable behavior is. And I think if you can get data-driven safety constraints out of this, which is something we have worked on a little bit, that could be a path towards providing for, let's say, safe adaptation to new situations. It's a big open problem, though. And I think that dealing with the combination of both the distributional shift that you encounter and just kind of the standard sort of physical safety constraints is an open problem still. Well, thank you for the answer. any more questions either from the room or from the Zoom. Okay, I think there's no more question. Let me stop the recording first and let's thank the speaker one more time. question let me stop the recording first and let's thank the speaker one more time.