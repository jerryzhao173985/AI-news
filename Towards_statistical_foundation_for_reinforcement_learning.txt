 Kevin Yeah, so this is going to be kind of a lecture with a different flavor than the ones that you've seen before. And in particular, it will be much more focused on understanding the theoretical foundations off. Um, some of the reinforcement learning algorithms and protocols that you've seen in class. Now, if we take a big step back and try to take a look at all the algorithms that you see in class and think about potential applications to the real world, you will see that there are still some challenges. One challenge is, for example, designing stable reinforcement learning algorithm. In particular, these may require some designing of certain tricks to ensure stability of the reinforcement learning algorithms. And often it translates into tuning some hyperparameters to achieve a certain performance. Now, another key issue about applying reinforcement learning paradigms to the real world is that efficiency in general of enforcement learning algorithms are extremely data hungry and they do require much more data than for example, algorithms that we commonly using supervised learning. Then there is the issue about generalization. Often a specific algorithm is tuned and and and he learns on a specific task. But it is much more difficult to design general class. General purpose algorithms that can perform well across completely different tasks. Um, and another issue is computational efficiency. If you try to do some of the homeworks in the class, you will see that, uh, sometime it takes quite a long time to train. Now, all these issues, um, are kind of specific to reinforcement learning. But they are issues that sort specific to reinforcement learning, but they are issues that sort of prevents reinforcement learning from being applied more broadly. In real world problems where issues like stability, convergence and sample efficiency becomes really fundamental. In particular, if you're interacting in the real world. Somehow you would like some predictability for what the algorithm is going to do. And also samples are generally quite expensive because they amount to interaction with your world. And so in this talk, we will try to take sort of a step back and try to understand some of the foundations for the reinforcement learning algorithms. And why do we want to develop sort of a theory for for reinforcement learning? Well, Perhaps the most basic motivation is that really the key basic protocols that people use in reinforcement learning are really algorithms that are motivated by theory, for example, value iteration. Policy iteration, upper confidence bound exploration, reinforced policy gradients. Those algorithms are all algorithms that at least they have. They are somewhat inspired by theory and oftentimes they come with guarantees at least in their simplest form. There are also some success stories of translating algorithms that were developed from theory into something more practical. One example is randomized least square value iteration, which you might know as a bootstrap DQN. Move over here, we can give you some consideration that apply not just to a specific problem, the one that you're trying to solve, but maybe more broadly to a wide, wide class of problems, and I would say more broadly to the field of our air. And also they help uncover fundamental limits, for example, things that you cannot do. And we will see an example of that today. Now, what question would we like to ask from a theoretical point of view? Well, ideally, we would like to have some form of guarantees for an algorithm that we are starting. For example, if you are proposing a new algorithm, you would like to understand whether it converges and that's kind of the primary concern that you might have when you go ahead and you want to apply it to a real problem. You would like to understand how to choose the hyper parameters, whether there is any sort of ways of formula or trade off that you need to make. Another question is how much data does the algorithm need to collect in order to achieve a certain level of performance. So how many interactions and also you might be concerned with things like computational complexity and so running time of the algorithm. This is what you would like to study. But in reality, um, answering those questions is extremely challenging. It's extremely difficult. Um, for example, for most of the deep parallel algorithms, it's not even possible to prove convergence because at the end of the day, the basic temporal difference scheme or to deal with experience replay and target networks. They are not always guaranteed to converge. And so immediately we have sort of a challenge. And it turns out that answering those questions is generally extremely difficult. And so what you will see today is that there is kind of a huge gap right now between the practical algorithms that you've seen in the class and some of the consideration that we will go through today. But at any rate, I will focus mostly on understanding the statistical aspects of a rail. And so how many samples do you need to learn a certain problems? And I will look into sort of three different macro topics. One is about trying to understand what the reinforcement learning problems are easy and harder. And whether we can learn faster on easier problems. And then I will focus on understanding the interplay between our algorithms and function approximation. So the issue of generalization. I will talk briefly about statistical limits. What you cannot do with the reinforcement learning algorithms and also briefly about offline reinforcement learning. Now let's get to sort of the first part understanding what problems are easy and what problems are hard. The setting that we consider here is the exploration problem. I think most of the class that that you've gone through is about exploration algorithms. Think about the standard online setting, DQN, all these. And so in this setting, you have an enforcement learning agent that is starting with an empty data set, and there is an interaction for H steps until the end, for example, of a game. And this interaction is ongoing, and it continues for a number of episodes. And you would like to measure how quickly our enforcement learning agent is learning. Now, intuitively, the reinforcement learning agent starts with a policy that might be suboptimal. If it is playing Atari, the first policy is going to be bad if you start with an empty data set, but then progressively it is going to learn and play policies that are better and better. What we would like to do is to measure the performance of the algorithm and the standard way to do it. We try to move this thing. The standard way to do it is to define a quantity that is called regret, which you might have seen in class. And it's really the sum of the suboptimality gaps of the policy played by the agent. Intuitively, at least if the problem is easy, an algorithm that is learning will approach in terms of performance the value of the optimal policy. But it will start in a way that it doesn't know much. And so the initial value of the policy that it plays are going to be low. And if we sum all the sub optimality gaps as a function of the episode. Well that would amount to computing the integral of discover. So the area shaded in orange. And that's what we call regret of the algorithm. Our goal will be to try to design an algorithm. The minimizes the regret. Now, in most cases, you can do that. For example, in deeper L, it's not not so clear that you can do that. And so we will focus on problems that are for the first part of today with small state and actual spaces so problem where we have a tabular representation. And if we go back to maybe 2010 2011 and subsequent years. In the foundations of a red, there was a huge pusher to try to design algorithms that could be as efficient as possible on tabular problems in particular, and several algorithms have been proposed and these they had some form of regret bounded. There was a function of the state and action space in particular is cardinality, the horizon and. And the number of episodes. Those are the great bounce are useful because they apply. broadly to any problem that is a macro decision process, you don't need to worry about the specific of the problem as long as you have. You know, find a state in action space. You have a guarantee on these algorithms. And this is also their limitation in the sense that it is not clear whether a certain algorithm would perform better or worse if the problem had a certain structure. And this is what we see in practice that the performance of reinforcement learning algorithms varies greatly, even for the same algorithm on problems that are quite different. And so we would like to start and try to derive some systematic understanding of what problems are difficult and what problems are easy to explore in reinforcement learning. Now. are easy to explore in the reinforcement learning. Now from a historical point of view, there has been a lot of effort into improving those regret bounds until essentially we got one algorithm that in terms of worst case performance it was unimprovable, meaning that it had a performance guarantee across all problems that was as good as possible given the lower bound that we knew, meaning that the performance is not improvable without any limit. There is a fundamental limit that you cannot surpass. At the same time, we know that there are classes of problems that are very different from the type of contrived construction that creates the lower bound. One example is a problem that has no dynamics or weak memory. A problem that has weak memory is a problem where the action that you took in the past, they have really little impact on your state. Think about a recommender system, which is a type of contextual bandit problem. Well, that is a situation in which this weak memory sort of arises. In a recommender system, think about a customer coming to Amazon. If you make a bad recommendation, intuitively you might make a certain customer unhappy, but this won't affect the next customer that you see. And so that's a problem of uh weak memory and for bandit problems we do know that there are specific bandit algorithms to take advantage of the structure and they are able to learn much faster than uh classical macro decision processes likewise problems that are deterministic are generally much easier. So it's essentially a search problem. Likewise, problems where you can only move locally in the state and action space are generally problem that are easier. Because if you make a mistake, you can still recover somehow. One example is mountain car. Now, the question that we ask is if we treat these problems as tabular problems where we have an explicit representation of the state and action space and the dynamics, what do the disease problems have in common? Can we identify some common characteristics and try to measure how hard they are? And can we learn faster if the problem belongs? If the actual problem instance that we face belongs to some of these subclasses. Well, we gave a positive answer to this and we proposed them first the problem dependent complexity and then an algorithm based on that that had certain specific properties. First of all, we propose some problem dependent complexity measure. They characterizes the complexity off. Different reinforcement learning problems. In particular, it is defined by the interaction off the system dynamics and the value of the optimal policy. It is defined as the variant of the next state optimal value function. And this is not something that the algorithm can compute if you do not know the actual market decision process because the optimal value function is unknown and the dynamics are also unknown, but nonetheless, you can design an algorithm that has a performance bound that scale with this quantity, which is generally unknown and the algorithm doesn't need to know that. As a result, the algorithm is able to match the best performance for tabular macro decision processes, meaning that it is minimax optimal. It is an improvable, but compared to the state of the art, it can also attain the optimal performance. Um, if the problem belongs to a certain class of easier problems, for example, if it is a contextual bandit problem, then the algorithm automatically matches essentially the performance of basic you CB on contextual bandits. And in addition to being an analytically small on certain problems and subclasses, you can evaluate the quantity numerically and. On a PR here is on on problems that people have considered before. It takes a value that is much smaller than sort of a worst case value that was suggested by prior bounds so essentially it is a quantity that, um, it is bought analytically small on problems that we care about, but also numerically smaller on problems that have been considered before. Now I want to pose one second and ask if there is any technical question on this part before I move ahead. The intuition, well, it really depends on the type of problem. So for example, if a problem is weak memory, it's a contextual bandit problem. What happens is that a mistake that you might make in a certain state, it doesn't really have long term consequences. And so the next state value function. And it wouldn't be too much different across different states. And so essentially this quantity has to be small. You might make an error, but you only lose with the current customer. Right? You don't mess up the entire long-term plan, right? And so this quantity end up being small. are long term plan right. And so this quantity end up being small think about as being. Some challenge in estimating the. The effect of transitions, but the transitions can be highly stochastic. For example, again in Venice, they are highly. But still, there is not much variability in the value of the state that you end up with. In that case, it's going to be small. The supremum, you can relax it as expectation over trajectories. It is supremum in the actual work about you. Okay. Um, I want to give one slide that is perhaps a bit more technical. Um, about how do we go about achieving something like this? Um, well, exploration generally is typically achieved. At least for probably efficient algorithms by adding an exploration bonus to the experience experience reward. Think about the Q and exploration there is done with Epsilon greedy, at least in the most basic form. But if you want more sophisticated schemes, think about you CB in in many algorithms. Normally what's done is a bonus is added. Now the bonus can take different forms. The most basic one that prior art was using is something that scales. Uh, with the inverse of the number of samples, it comes from often inequality. But this type of exploration bonus is essentially problem independent, meaning that it's not tied to any particular feature of the MDP. And so the algorithm would explore in the same way, regardless of the problem. And this won't give rise to problem dependent bounds. Now the ideal choice that one would like to make is to use some form of Bernstein based concentration inequality, which does indeed contain something very similar to the quantity that we want. It would give rise to problem-dependent bounds, but there is one issue, that in general, the optimal action value function, you don't know what it is. And the transition dynamics, you also don't know what it is. So although this choice of the bonus will be ideal, it will not practically give rise. It's not something that you can do in practice. The way around it is sort of intuitive, is to try to use the empirical dynamics and some empirical estimate of the optimal value function. But there are several challenges that arises if you try to do that. The main challenge is that generally those quantities, they are unknown. Think about when you start initially, you know very, very little about the dynamics. And so you have essentially no way to guess what these quantities are. And if you take the wrong guess, essentially, the algorithm might not be optimistic enough. It might not explore enough. And it would just not find a good policy. So what you have to do is rather to introduce some correction terms. Thankfully, those correction terms that try to correct for your wrong estimates. They decay very quickly, so they decay at a faster rate. And so it is it is as if the agent was applying a sort of the correct Bernstein based concentration inequality, but with one correction term that is decaying very quickly. And the challenge here lies in estimating the size of the correction, in particular because we have to correct some value function that is different from the optimal one. And estimating those errors, it requires estimating however propagates. Um, through the MDP from states that perhaps we haven't even visited them. And this choice essentially gives rise to those problem dependent bounds. Now, rise to those problem-dependent bounds. Now, this is good because it does give you some initial strong understanding of whether it's possible to adapt to the problem difficulty and whether it's possible to be, at the same time, minimax optimal but also instance optimal on a variety of problem classes that we are interested in. But the big limitation here is that, of course, this thing applies only to small state and action spaces. In practice, we would like to tackle problems that have a very large, potentially continuous, um, state and action space. And we clear what you've seen in the class is always in this second category. As soon as you start using any form of function approximation, you are in this category. And so the next question that we will try to understand is what can we say about reinforcement learning with function approximation? And the answer will turn out to be a bit more negative than here. Here we made some sort of positive progress. But here we will see that when you start to talk about reinforcement learning with function approximation, even problems that seem to be easy, they might be very challenging. And so to do a quick recap, practical problems, they always have a state space that is extremely large. Most states are never visited. What we would like to do is to introduce some form of function approximation. They can add generalized knowledge from the states that we have seen to states that we have not yet yet observed. And the hope is that we do not need to learn what to do in every state. Rather, we need only a number of samples that is roughly of the same order as the number of parameters in our model. Now the observation that the full clue observation if you want that we have is that reinforcement learning algorithm they use function approximation. They still need a lot of samples compared to supervised learning. And so we would like to ask a very basic question, whether reinforcement learning is, for example, fundamentally more difficult than classical supervised learning. And in order to study this question, we consider a setting that is very similar to the offline reinforcement learning setting that you have seen in the second part of the class. In the offline reinforcement learning setting, you have some data set that is available and it consists of state actions reward and successor states and we try to ask questions about, for example, policies that might be different from the one that generated the data set. You may, for example, want to try to identify the optimal policy or you may try to do off policy evaluation. The specific setting that we consider is one in which we allow some sort of data collection with a static distribution before and the reason to do that to allow for some flexibility is because if the data set is poor, intuitively, we cannot do much and that's not the algorithm fault. It's just the data set. Maybe I have just data on a single statement. So we do consider a case in which you can do some form of data collection with a static policy beforehand, and then we try to understand whether we can successfully predict the value of a different policy, for example, or extract the value of the optimal policy. Now our expectation is that If the action value function as a simple representation, for example, if the action value function, um, as a linear expansion and perhaps we even know the feature extractor. Then this should be an easy problem. Why well, it's just by analogy with linear regression. If you are solving a regression problem and I give you a feature map and I promise that the problem is realizable. So the target do have some linear expansion, perhaps with some noise. Then you can open a textbook in statistics and you will see that Standard linear regression can learn this problem very quickly. However, in reinforcement learning, even problems that are linear, they don't seem to be so easy. In particular, there have been examples of divergence of classical TD and fitted Q, even on problems that are linearly realizable. And in fact, if you take a look at the analysis that that are available for some of the basic algorithms and protocols, you will see that they all make some assumptions that seem to be much more stronger than just realizability. And so as a matter of fact, we don't know in 2020 2021 whether even the simplest linear setting is something that we can provide stable algorithms for. Can we provide an algorithm? Um, that, for example, converges and converges. Yes, because you can use electricity. But can we have any guarantee about Um, for example, the amount of samples that are required to learn even in this simple setting, which is the first step after tabular problems and really to understand what's happening. Um, you need to compare supervised learning with reinforcement learning, and the key difference is whether you're trying to make predictions one time step or for many times that This is because if you're trying to make predictions for one time step and you start with a data set that you might have recollected in an intelligent way. Um, well, if you're trying to just predict the first reward and you have the promise that the reward function is linear, then we know that linear regression solves this problem very, very quickly. So we know an algorithm, and we know guarantees as well. And this is the most basic machine learning algorithm that you can think of. However, our question is, what happens if we want to predict the value of a policy for multiple time steps with the problem with the with the guarantee that that value is actually realizable, meaning that we have a feature extractor that correctly predicts the value of the target policy for some data parameter. It turns out that this problem is, in the worst case, extremely difficult, meaning that as opposed to supervised learning, you can find problems where you have this beautiful linear model. And yet, any algorithm would take a number of samples to make the correct predictions that is exponential in the dimensionality of the feature extractor. And when I say predictions, you can intend this broadly, meaning that the answer would remain the same. If you are trying to, for example, identify an optimal policy, I want a policy that does better than random. You still need a number of samples that, in the worst case, might be exponential in the dimensionality of the feature extractor. And so we see that there is a strong separation between what is achievable in supervised learning, which is concerned with making predictions. And so if you want a one step prediction, um, and the reinforcement learning, which considers sequential processes as as the horizon becomes longer, Uh, the problems can become exponentially harder. This doesn't mean that all problems are exponentially harder, but it does tell you that even problems that appear to be simple the problems that should be linear. And so they should be easily learnable. You will not be able to find an algorithm that has guarantees even on those problems. And so for you, in order to learn for an algorithm to learn, there has to be some additional special structure. And indeed, this is something that we sort of see that in the poor sample complexity is a major issue in a relevant and this issue is also related to divergence. But the contribution here is really to identify that those issues are algorithm independent. They're information theoretic, meaning that there is some fundamental hardness in the reinforcement learning problem that applies broadly to all algorithms that you can come up with. You will not be able to find an algorithm that is able to solve all problems, even if they are as simple as linear. And this issue has been studied more broadly by some other sort of important papers, and some have sort of similar result. And if you want to sort of reinterpret this second section, you can also take a look at that from the point of view of online RL. I might have an action value function. Think about what you have in DQN. And instead of having a deep neural network, you just have a simple linear map. And I promise to you that the problem really does have a linear action value function. Still, it will not be able to find an algorithm that can learn polynomially fast in a problem that is linear. And so the main takeaway here is that linear regression is easy to start, but the equivalent in reinforcement learning from a model-free point of view is already out of reach. And so we have to be sort of not too optimistic about the type of problems that we can solve. And there has been, indeed, a big effort trying to understand what additional conditions are necessary. Um, in order to have polynomial sample complexity as we have for many statistical algorithms in statistic. Now, before we move forward, is there any question on this second section? Yeah. Yeah. I think the thing that is really tricky is that this is really a model for the point of view, right? So we're looking at whether we have enough information on the two values. But somehow you can have problems that are extremely complex. But the action value function ends up being simple. It's an it ends up being sort of linear. So the actual content example has essentially. Every word function that is very complex. It's like a value normal networks that is no zero only in a in a very, you know, hidden area of the state space, which is exponentially larger. The dynamics are very complex, and the dynamics are sort of engineered in a way that they linearized the reward function in the sense that once you do many steps of the Behrman backup, you end up with an action value function that looks linear. And so it looks like the problem is easy because that thing is really linear. But what you're really trying to do is to identify where the reward function is no zero in a sort of an exponentially larger sphere. I don't know if I can say much more than this, but it's really related to the high dimensionality that you have. There's a lot of space in high dimension. If you take random vectors in high dimension that will almost always be orthogonal and so you can sort of hide information in very high dimension. It's not something that is obvious in two or three d. You really have to go high dimension. Yeah, the policy pie is fixed. It could be, you know, think about predict the value of the optimal parts. It could be fixed, or it could be, you know, the optimal one. I do want to spend one slide. Talking about. Indeed, what happens with more general function approximation. In terms of positive result. Well, if you open some book about statistics, I dimension of statistics, at least for regression, You will see that there are performance guarantees that are a function of the very function class you're using. So if you're using kernel methods convex functions or other things you will have some performance bound some trade-off between approximation error and statistical complexity and the statistical complexity is normally expressed in sort of more. In notions like about the market complexity BC dimensions and other things. But the same is not sufficient in a way, so it looks like. The interplay between. By man operator and the function, the very same function class that you use to model the action value function for TD methods, but the interplay is really kind of important and so on what people have focused on on to understand some foundations of our health. It's not just about the complexity of the function class. This is not sufficient like we saw before we have a linear map and that is already too hard. Well, there has to be something that makes the problem sort of learnable and that's really the interaction between Bellman operator and the actual value function the reason why that's essential for TD method is that you're taking an actual value function you created the Bellman backup out of it and you're fitting sort of the same and you want that to be zero and so the interaction becomes critical and so many many notions of the same, and you want that to be 0. And so the interaction becomes critical. And so many notions have been proposed to try to understand in what cases can you do this sort of learning in a way that is stable and statistically efficient. But I won't go into that. Instead, I'm going to jump to Um, some offline of enforcement learning. Um, offline of reinforcement learning. You've seen it already in the class, but just to do a quick recap. It's with there's really a lot of data out there, so we would like to leverage them. How can we do so without collecting further data? And the setting is the same as that that you've seen in class. We have Historical data set of state actions were rewarding successful states. And the task is how do we find the policy with the highest value? What does it even mean to find the policy with the highest value given a data set? Well, the highest value, of course, is the policy that has that is the optimal policy, but Your data set may contain no information about the optimal policy. And so It's like we have to make sort of a best effort. Um, uh, some best effort in trying to identify a good policy and lower our risk expectation. Um, and perhaps not find the optimal policy. The main challenge. I think you have discussed this in class as well. Is that of distribution shift? Meaning that Um, The data set. Well, the best case scenario, which never happened is one in which your data set as Uniform samples all over the state and action space. In which case you could just try to evaluate policy by one by two and by three and pick up the best. But normally what you've given is, uh, um, project of is that they might be for example from humans and so they're generally narrowly concentrated. And that's what we call problem of partial coverage. And in the example here, the data set may have a lot of information about I want no information about pie two and some information about pie three and somehow you have to come up with and choose between the three and figure out which policy is the best. And the thing that they want to sort of I like today is how do we even measure this? Um, coverage? How much information the data set contains to find a good policy. And intuitively, the way to solve this problem is precisely what you've seen in class. Or actually, there are two ways, if you want. One is to try to stay close to the policies that generated the data set some form of Behavioral cloning. Another way is to attempt to estimate the uncertainty about your predictions. Generally, your data set. As a Um, is generated by policies, certain policies that are sort of narrowly concentrated and and they give you data about state actions, rewarding transitions. And you would try to sort of fit some form of model and try to use the model to make predictions about the value of other policies. Now the model doesn't actually need to be a model. You may do this in a model freeway, but you still using some data that has been generated by some policies and make predictions about other policies. Now, of course, what you would like to do is to pick up the policy that has the highest value, but that's not known. Um, instead, you would like to return a policy. It looks like it has good value, but that you're also reasonably confident about. And so one way to look at the flying around is As a procedure that tries to find some optimal trade off between value of the policy that is returned and the uncertainty about this policy. Think about some bias bias variance trade off. In statistics, you would like to have an algorithm that has sort of an optimal bias variance trade off. Um, the bias is generally unknown. The variance you can try to estimate. In offline of L. There is sort of a similar, uh, notion. If you want, you would like to balance The value of, um, the policy that you return, which is unknown to you. With is uncertainty. And so guarantees for offline reinforcement learning algorithm. uncertainty and so guarantees for offline reinforcement learning algorithm. Uh, they generally look something like this. One algorithm should return with very high probability. The best trade off between The value of the policy that he returns and his uncertainty, which Essentially amounts to finding the point with the highest lower bound in a sense of flying the reinforcement learning, the one that you've also seen in the class in some way, they try to get to this optimal trade off. Now, one big question is what is this sort of constant see that depends on the policy? Well, if you have seen concentration in the qualities in statistics, you might be already familiar with the term 1 divided by square root of n. It's what arises from, for example, opting inequality. But here there is an extra coefficient that depends on the policy, which should encapsulate the distribution shift. Now, this coefficient depends on the policy, which should encapsulate the distribution shift. Now, this coefficient depends on the actual algorithm, and it depends, for example, on the function classes that you're using and the interaction within the Bedman operator. And as a concrete instantiation, you can take, for example example softmax policies think about those that arise from a natural policy gradient and again for simplicity linear action value functions and those are two distinct parameters and you can design algorithms that. Essentially try to solve this offline reinforcement learning problem, and they will have some guarantees that are precisely of this former where these coverage coefficient As a certain analytical expression and the analytical expression highlights really the interplay between the information that is contained in the data set and really the interplay between the information that is contained in the data set and The target policy that you're trying to estimate in particular of the information contained in the data set is reflected in the covariance metrics, which is a somewhat familiar object from statistics linear regression. You compute some covariance metrics. The covariance contains the amount of information that you know about the problem and these interact with a certain norm in his inverse with the expected feature over. The target policy that you are considering for the optimization. This quantity you know, but this one is generally not computable, right? So this sort of tells you how to interact. To create confidence interval for off policy valuation that you can use to find a good policy. What is sort of surprising and perhaps not surprising, but important about this is that This coverage, which is also called consent ability. Um, it doesn't really It doesn't have an expression in the state in action space. If you open some of the papers that do statistical analysis, you would often find a ratio between busy distribution. This county busy distribution of the target policies versus the behavioral policy you would often find a ratio between busy distribution discounted easy distribution of the target policies versus the behavioral policy. And that is a ratio in the state and action space. This one has none of that is so projected down to a lower dimensional feature space where coverage can indeed be sort of much larger. Think about having a covariance matrix that is the identity. That would certainly make the coverage coefficient be very small. I don't think I want to talk about sort of how we achieve this and sort of the technicalities. I think the important part is how would, for example, a guarantee in a fly in a rail look like in terms of actual statement, which is what you saw in the prior slide. But if you want at a very high level, we're trying to avoid penalizing, um, actions directly and we want to retain a very sort of low statistical complexity and and we operate in the parameter space to compute this confidence intervals and and all this is put into a big actor critic. Algorithms that uses. Um, natural policy gradient and some pessimistic version of. TD with target networks where the parameters are moved in a way that computes a pessimistic solution. I'm going to. Keep the algorithm. Um, now one limitation of this study that you've seen is that it applies to the linear setting. Of course, the question is what happens if I use a richer set of functions, for example, um, offline reinforcement learning with more general function approximations such as the ones that you've seen in class. Can we give any guarantees for those? The answer is, unfortunately, there is a huge gap in the sense that the type of algorithms that you've seen in the class is very difficult to prove guarantees for them because they may not converge. There are variants that are sort of, in a sense sense that you can provide guarantees for the big problem is that it's not clear how you would implement it. And that's kind of an issue of all over L with function general function approximation. If you want guarantees, it's not clear how you would come up with an algorithm that you can actually implement. And so oftentimes what is analyzed is sort of a conceptual version that is not the same as the actual algorithm that is implemented in practice. Now before I head to the conclusion, any question on this sort of third part? Yeah, of course. Horizon. So this movement? Well, the covariance, if this is a finite horizon problem, the covariance can really change to time steps. Um, is the covariance of the features so some of fee fee transfers. p transpose. feature feature extract. same as linear regression. The same object. What do you mean by epsilon optimal policy? This won't be this won't be optimal right because this offline are l so it really depends on your data. Policy that you find may be very crappy if your data set doesn't have good information. Suppose your data set is just from a policy that is narrowly concentrated and the behavioral policy is bad and This feature matrix is like rank one is so concentrated in one direction. Then that doesn't really tell you much. And so you won't be able to find a good policy. But somehow this is sort of reflected in precisely that statement, right because policies that are very good. They would have a coverage coefficient is very large. And so you will not know their value and no algorithm would return that. Yeah. Um, no, I think, I think if you want to, this is the epsilon that you're talking about, right? This is the policy that we will return. You can always think about the optimal policy in the supreme. I want to evaluate these expression are the optimal policy. I can do that. But then this guy will become the value that sort of coverage about the optimal policy. And so this is your your Epsilon, right? This is Epsilon suboptimal compared to the optimal policy. But Epsilon can be huge. Basically, I'm telling you the value of Epsilon given the data set that we have. This is really a way to measure how much information are contained in the data set. And as a result was the performance that you can expect. Um, the initial state, I would say. The value of the policy at the initial state. That's sort of the one that you care about. And your estimates may be more often in states that, for example, you don't visit, or even in states that you visit, but they might compensate. What you really care is performance at the starting point. Right, right, right. So I think you have seen something similar with CQL, I think. Basically, on X, let's plot. Let's put different policies to be clear. You're going to have uncountably many, right? But let's, you know, put them on a graph and on the why we're going to plot the value of the policies. Yeah, yeah, yeah, yeah. Expected this country some of the world. Now what you wish you knew is the actual value of the policies, right? Yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeah, yeahuitively, if you're doing model-based RL, you can try to fit some model and try to use that to make predictions. So your model may be good, it may be bad. It might generally be good to make predictions about value of policies that generate the data set. It might be very bad to predict value of different policies. You might not use a model-based version, and you may do something different. For example, you may adopt a model free approach like CQL. Um, and intuitively. If you could what you would do is the following try to come up with an estimator for the value of different policies and try to measure also the uncertain. The uncertainty is really sort of this band here, right? And this curve may move up and down depending on the data center. But you would like to try to estimate the uncertainty about your predictions. Intuitively, the uncertainty will be smaller for on policy evaluation for the very policy that's generated at the opposite. We have a bunch of data. It just take the average. But it will be very bad for a policy that is very, very different. The visits completely different areas of the state in action space. Your data is narrowly concentrated. You have no idea about the policy that does something in a completely unknown area of the state in action space. So even if that policy by doing, you know, fitted kill, it looks good. You have to take into account that you're extremely uncertain about this value. And so somehow you would like to penalize it. And so you would like to say, Oh, this policy like this. I'm too uncertain. Like I'm going to sign it a very low value. And if I do this procedure, um, the optimal trade off is to maximize this lower bound, the lower a lower bound on the performance of the policies. And this gives you sort of abstractly these expression here, which will become concrete as soon as you consider a specific algorithm and a specific type of function class and that will determine the the CPI. But. Yeah. Yeah. Yeah, yeah, yeah, yeah, yeah. Of course, thank you. I think what's important here, if you want one takeaway, is really how would a guarantee of an offline algorithm look like. It would look something like this, some trade off between values. Of course, you want the highest value, but policy that have high value. You might be very uncertain about that. So there's going to be some some trade off. Um, and just to make it if you want more clear. Um, for the policy that are in the data set. Generally, you have a lot of data, right? And so this is going to be something like one. You have a lot of data. So n is bigger. This quantity is small. And so what this expression tells you is that you should do better than the policies that generated the data set. If you design the algorithm correctly, this is sort of the minimum that you would expect. You want to do better than behavior clone. And this expression test exactly that if I put pie as the heavy old policy that generated the data set CPI will be small. It's going to be one. This expression will be smaller. And this tells me I do better than the heavyweight chronic, which is what we would expect. Okay. Time to sum up. We have seen sort of three things. One is most problems in a row, they're not worst case. They belong to a much easier class of problems. We have seen that as soon as we move to function approximation, RL is much more difficult than standard supervised learning. And then we have seen what type of guarantees we can obtain for offline reinforcement learning algorithms. And to conclude, I think after the presentation, this is much more clear. There is a huge gap between sort of theory and practice. I think working at the intersection is, of course, difficult, right? Because you have to sort of please both communities, but in my head, make reinforcement learning more applicable in the sense that they're going to be compromises to be made. You want sort of beat any benchmark or, you know, top any benchmark, but you might be able to come up with some algorithm that has some sort of analysis and at least in simple cases, some ability guarantees. And this will be kind of critical in order to apply reinforcement learning to very different problems. You would feel much more confident to apply an algorithm if it is backed by some form of guarantees that apply even in a restricted set. And generally, theory will not tell you how to tune hyperparameters and all that. So it will not necessarily inform you on the specifics of any given application, but it can give you sort of more broad insights and foundation that apply more broadly To the field that we have seen some On the mental lower bounds before. Um, and, um, yeah, I think with this, I conclude and this is everything I had for today. So thank you for your attention. I'm going to ask if there is any final question. Thank you for coming here.